{"cells":[{"cell_type":"markdown","source":["##Spark DataFrames"],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import *  # Necessary for creating schemas\n","from pyspark.sql.functions import * # Importing PySpark functions"]},{"cell_type":"markdown","source":["###The 'groupBy' Function and Aggregations\n\nThe `groupBy()` function groups the DataFrame using the specified columns, then, we can run aggregation on them. The available aggregate functions are:\n\n- `count()`: counts the number of records for each group\n- `sum()`: compute the sum for each numeric column for each group\n- `min()`: computes the minimum value for each numeric column for each group\n- `max()`: computes the maximum value for each numeric column for each group\n- `avg()` or `mean()`: computes average values for each numeric columns for each group\n- `pivot()`: pivots a column of the current DataFrame and perform the specified aggregation\n\nBefore we get into aggregations, let's load in a **CSV** with interesting data and create a new DataFrame.\n\nYou do this with the `spark-csv` package. Documentation on that is available at:\n- https://github.com/databricks/spark-csv\n\nThe dataset that will be loaded in to demonstrate contains data about flights departing New York City airports (`JFK`, `LGA`, `EWR`) in 2013. It has 336,776 rows and 16 columns."],"metadata":{}},{"cell_type":"markdown","source":["There are more time-based functions:\n- `date_sub()`: subtract an integer number of days from a *Date* or *Timestamp*\n- `date_add()`: add an integer number of days from a *Date* or *Timestamp*\n- `datediff()`: get the difference between two dates\n- `add_months()`: add an integer number of months\n- `months_between()`: get the number of months between two dates\n- `next_day()`: returns the first date which is later than the value of the date column\n- `last_day()`: returns the last day of the month which the given date belongs to\n- `dayofmonth()`: extract the day of the month of a given date as integer\n- `dayofyear()`: extract the day of the year of a given date as integer\n- `weekofyear()`: extract the week number of a given date as integer\n- `quarter()`: extract the quarter of a given date"],"metadata":{}},{"cell_type":"markdown","source":["###Joins"],"metadata":{}},{"cell_type":"markdown","source":["Joins are easily performed with Spark DataFrames. The expression is:\n\n`join(other, on = None, how = None)`\n\nwhere:\n- other: a DataFrame that serves as the right side of the join\n- on: typically a join expression\n- how: the default is `inner` but there are also `inner`, `outer`, `left_outer`, `right_outer`, and `leftsemi` joins available"],"metadata":{}},{"cell_type":"markdown","source":["Let's load in some more data so that we can have two DataFrames to join. The **CSV** file `weather.csv` contains hourly meteorological data from EWR during 2013."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object...\n","weather_schema = StructType([  \n","  StructField('year', IntegerType(), True),\n","  StructField('month', IntegerType(), True),\n","  StructField('day', IntegerType(), True),\n","  StructField('hour', IntegerType(), True),\n","  StructField('temp', FloatType(), True),\n","  StructField('dewp', FloatType(), True),\n","  StructField('humid', FloatType(), True),\n","  StructField('wind_dir', IntegerType(), True),\n","  StructField('wind_speed', FloatType(), True),\n","  StructField('wind_gust', FloatType(), True),\n","  StructField('precip', FloatType(), True),\n","  StructField('pressure', FloatType(), True),\n","  StructField('visib', FloatType(), True)\n","  ])\n","\n","#...and then read the CSV with the schema\n","weather = \\\n","(sqlContext\n"," .read\n"," .format('com.databricks.spark.csv')\n"," .schema(weather_schema)\n"," .options(header = True)\n"," .load('s3://bigdatatoolscloud/weather.csv'))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Have a look at the imported dataset\ndisplay(weather)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# We need those `month`, `day`, and `hour` values back\nnycflights = \\\n(nycflights\n .withColumn('month', month(nycflights.timestamp))\n .withColumn('day', dayofmonth(nycflights.timestamp))\n .withColumn('hour', hour(nycflights.timestamp)))\n\n# Join the `nycflights` DF with the `weather` DF \nnycflights_all_columns = \\\n(nycflights\n .join(weather,\n       [nycflights.month == weather.month, # three join conditions: month,\n        nycflights.day == weather.day,     #                        day,\n        nycflights.hour == weather.hour],  #                        hour\n       'left_outer')) # left outer join: keep all rows from the left DF (flights), with the matching rows in the right DF (weather)\n                      # NULLs created if there is no match to the right DF"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# Notice that lots of columns created, as well as duplicate column names (not a bug! a feature?)\ndisplay(nycflights_all_columns)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# One way to reduce the number of extraneous\n# columns is to use a `select()` statement\nnycflights_wind_visib = \\\n(nycflights_all_columns\n .select(['timestamp', 'carrier', 'flight',\n          'origin', 'dest', 'wind_dir',\n          'wind_speed', 'wind_gust', 'visib']))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# Examine the DataFrame, now with less columns\ndisplay(nycflights_wind_visib)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["Let's load in even more data so we can determine if any takeoffs occurred in very windy weather.\n\nThe **CSV** `beaufort_land.csv` contains Beaufort scale values (the `force` column), wind speed ranges in *mph*, and the name for each wind force."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object... \n","beaufort_land_schema = StructType([  \n","  StructField('force', IntegerType(), True),\n","  StructField('speed_mi_h_lb', IntegerType(), True),\n","  StructField('speed_mi_h_ub', IntegerType(), True),\n","  StructField('name', StringType(), True)\n","  ])\n","\n","# ...and then read the CSV with the schema\n","beaufort_land = \\\n","(sqlContext\n"," .read\n"," .format('com.databricks.spark.csv')\n"," .schema(beaufort_land_schema)\n"," .options(header = True)\n"," .load('s3://bigdatatoolscloud/beaufort_land.csv'))\n"," "],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["# Have a look at the imported dataset\ndisplay(beaufort_land)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# Join the current working DF with the `beaufort_land` DF\n# and use join expressions that use the WS ranges\nnycflights_wind_visib_beaufort = \\\n(nycflights_wind_visib\n .join(beaufort_land,\n      [nycflights_wind_visib.wind_speed >= beaufort_land.speed_mi_h_lb,\n       nycflights_wind_visib.wind_speed < beaufort_land.speed_mi_h_ub],\n       'left_outer')\n .withColumn('month', month(nycflights_wind_visib.timestamp)) # Create a month column from `timestamp` values\n .drop('speed_mi_h_lb')\n .drop('speed_mi_h_ub')\n)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["# View the joined DF; now we have extra data on wind speed!\ndisplay(nycflights_wind_visib_beaufort)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# We can inspect the number of potentially dangerous\n# takeoffs (i.e., where the Beaufort force is high)\n# month-by-month through the use of the `crosstab()` function\ncrosstab_month_force = \\\n(nycflights_wind_visib_beaufort\n .crosstab('month', 'force'))\n\n# After creating the crosstab DataFrame, use a few\n# functions to clean up the resultant DataFrame\ncrosstab_month_force = \\\n(crosstab_month_force\n .withColumn('month_force',\n             crosstab_month_force.month_force.cast('int')) # the column is initially a string but recasting as\n                                                           # an `int` will aid ordering in the next expression\n .orderBy('month_force')\n .drop('null'))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# Display the cross tabulation; turns out January was a bit riskier for takeoffs due to wind conditions\ndisplay(crosstab_month_force)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["###User Defined Functions (UDFs)\n\n**UDF**s allow for computations of values while looking at every input row in the DataFrame. They allow you to make your own function and import functionality from other **Python** libraries."],"metadata":{}},{"cell_type":"code","source":["# Define a function to convert velocity from\n# miles per hour (mph) to meters per second (mps)\ndef mph_to_mps(mph):\n  mps = mph * 0.44704\n  return mps\n\n# Register this function as a UDF using `udf()`\nmph_to_mps = udf(mph_to_mps, FloatType()) # An output type was specified"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# Create two new columns that are conversions of wind\n# speeds from mph to mps\ndisplay(\n  nycflights_wind_visib_beaufort\n  .withColumn('wind_speed_mps', mph_to_mps('wind_speed'))\n  .withColumn('wind_gust_mps', mph_to_mps('wind_gust'))\n  .withColumnRenamed('wind_speed', 'wind_speed_mph')\n  .withColumnRenamed('wind_gust', 'wind_gust_mph')\n)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["###Writing DataFrames to Files\nWe can easily write DataFrame data to a variety of different file formats."],"metadata":{}},{"cell_type":"code","source":["# Saving to CSV is quite similar to reading from a CSV file\n","(crosstab_month_force\n"," .write\n"," .mode('overwrite')\n"," .format('com.databricks.spark.csv')\n"," .save('s3://bigdatatoolscloud/'))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["# Saving to Parquet is generally recommended for later retrieval\n","(crosstab_month_force\n"," .write\n"," .mode('overwrite')\n"," .parquet('s3://bigdatatoolscloud/'))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["###Useful Links"],"metadata":{}},{"cell_type":"markdown","source":["There are many more functions... although I tried to cover a lot of ground, there are dozens more functions for DataFrames that I haven't touched upon.\n\nThe main project page for Spark:\n\n- http://spark.apache.org\n\nThe main reference for PySpark is:\n\n- http://spark.apache.org/docs/latest/api/python/index.html\n\nThese examples are available at:\n\n- https://github.com/rich-iannone/so-many-pyspark-examples\n\nInformation on the Parquet file format can be found at its project page:\n\n- http://parquet.apache.org\n\nThe GitHub project page for `spark-csv` package; contains usage documentation:\n\n- https://github.com/databricks/spark-csv"],"metadata":{}}],"metadata":{"name":"spark-dataframes","notebookId":96485},"nbformat":4,"nbformat_minor":0}