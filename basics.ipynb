{"cells":[{"cell_type":"markdown","source":["##Spark DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["###Imports\n\nFor these examples, we just need to import two **pyspark.sql** libraries:\n- `types`\n- `functions`\n\nWe need `pyspark.sql.types` to define schemas for the DataFrames. The `pyspark.sql.functions` library contains all of the functions specific to SQL and DataFrames in **PySpark**."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *  # Necessary for creating schemas\nfrom pyspark.sql.functions import * # Importing PySpark functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###Creating DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["#### Making a Simple DataFrame from a Tuple List"],"metadata":{}},{"cell_type":"code","source":["# Make a tuple list\n","a_list = [('a', 1), ('b', 2), ('c', 3)]\n","\n","# Create a Spark DataFrame, without supplying a schema value\n","df_from_list_no_schema = \\\n","sqlContext.createDataFrame(a_list)\n","\n","# Print the DF object\n","print (df_from_list_no_schema)\n","\n","# Print a collected list of Row objects\n","print (df_from_list_no_schema.collect())\n","\n","# Show the DataFrame\n","df_from_list_no_schema.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Making a Simple DataFrame from a Tuple List and a Schema"],"metadata":{}},{"cell_type":"code","source":["# Create a Spark DataFrame, this time with schema\ndf_from_list_with_schema = \\\nsqlContext.createDataFrame(a_list, ['letters', 'numbers']) # this simple schema contains just column names\n\n# Show the DataFrame\ndf_from_list_with_schema.show()\n\n# Show the DataFrame's schema\ndf_from_list_with_schema.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Making a Simple DataFrame from a Dictionary"],"metadata":{}},{"cell_type":"code","source":["# Make a dictionary\n","a_dict = [{'letters': 'a', 'numbers': 1},\n","          {'letters': 'b', 'numbers': 2},\n","          {'letters': 'c', 'numbers': 3}]\n","\n","# Create a Spark DataFrame from the dictionary\n","df_from_dict = \\\n","(sqlContext\n"," .createDataFrame(a_dict)) # You will get a warning about this\n","\n","# Show the DataFrame\n","df_from_dict.show()\n","\n","# inferring schema from dict is deprecated, please use pyspark.sql.Row instead\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Making a Simple DataFrame Using a StructType Schema + RDD"],"metadata":{}},{"cell_type":"code","source":["# Define the schema\nschema = StructType([\n    StructField('letters', StringType(), True),\n    StructField('numbers', IntegerType(), True)])\n\n# Create an RDD from a list\nrdd = sc.parallelize(a_list)\n\n# Create the DataFrame from these raw components\nnice_df = \\\n(sqlContext\n .createDataFrame(rdd, schema))\n\n# Show the DataFrame\nnice_df.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Define the schema\nschema = StructType([\n    StructField('letters', StringType(), True),\n    StructField('numbers', IntegerType(), True)])\n\n# Create an RDD from a list\nrdd = sc.parallelize(a_list)\n\n# Create the DataFrame from these raw components\nnice_df = \\\n(sqlContext\n .createDataFrame(rdd, schema))\n\n# Show the DataFrame\nnice_df.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["###Simple Inspection Functions\n\nWe now have a `nice_df`, here are some nice functions for inspecting the DataFrame."],"metadata":{}},{"cell_type":"code","source":["# `columns`: return all column names as a list\nnice_df.columns"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# `dtypes`: get the datatypes for all columns\nnice_df.dtypes"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# `printSchema()`: prints the schema of the supplied DF\nnice_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# `schema`: returns the schema of the provided DF as `StructType` schema\nnice_df.schema"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# `first()` returns the first row as a Row while\n","# `head()` and `take()` return `n` number of Row objects\n","print (nice_df.first()) # can't supply a value; never a list\n","print (nice_df.head(2)) # can optionally supply a value (default: 1);\n","                      # with n > 1, a list\n","print (nice_df.take(2)) # expects a value; always a list"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# `count()`: returns a count of all rows in DF\nnice_df.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# `describe()`: print out stats for numerical columns\nnice_df.describe().show() # can optionally supply a list of column names"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# the `explain()` function explains the under-the-hood evaluation process\nnice_df.explain()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["###Relatively Simple DataFrame Manipulation Functions\n\nLet's use these functions:\n- `unionAll()`: combine two DataFrames together\n- `orderBy()`: perform sorting of DataFrame columns\n- `select()`: select which DataFrame columns to retain\n- `drop()`: select a single DataFrame column to remove\n- `filter()`: retain DataFrame rows that match a condition"],"metadata":{}},{"cell_type":"code","source":["# Take the DataFrame and add it to itself\n(nice_df\n .unionAll(nice_df)\n .show())\n\n# Add it to itself twice\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df)\n .show())\n\n# Coercion will occur if schemas don't align\n(nice_df\n .select(['numbers', 'letters'])\n .unionAll(nice_df)\n .show())\n\n(nice_df\n .select(['numbers', 'letters'])\n .unionAll(nice_df)\n .printSchema())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Sorting the DataFrame by the `numbers` column\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df)\n .orderBy('numbers')\n .show())\n\n# Sort the same column in reverse order\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df)\n .orderBy('numbers',\n          ascending = False)\n .show())"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# `select()` and `drop()` both take a list of column names\n# and these functions do exactly what you might expect\n\n# Select only the first column of the DF\n(nice_df\n .select('letters')\n .show())\n\n# Re-order columns in the DF using `select()`\n(nice_df\n .select(['numbers', 'letters'])\n .show())\n\n# Drop the second column of the DF\n(nice_df\n .drop('letters')\n .show())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# The `filter()` function performs filtering of DF rows\n\n# Here is some numeric filtering with comparison operators\n# (>, <, >=, <=, ==, != all work)\n\n# Filter rows where values in `numbers` is > 1\n(nice_df\n .filter(nice_df.numbers > 1)\n .show())\n\n# Perform two filter operations\n(nice_df\n .filter(nice_df.numbers > 1)\n .filter(nice_df.numbers < 3)\n .show())\n\n# Not just numbers! Use the `filter()` + `isin()`\n# combo to filter on string columns with a set of values\n(nice_df\n .filter(nice_df.letters\n         .isin(['a', 'b']))\n .show())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["###The 'groupBy' Function and Aggregations\n\nThe `groupBy()` function groups the DataFrame using the specified columns, then, we can run aggregation on them. The available aggregate functions are:\n\n- `count()`: counts the number of records for each group\n- `sum()`: compute the sum for each numeric column for each group\n- `min()`: computes the minimum value for each numeric column for each group\n- `max()`: computes the maximum value for each numeric column for each group\n- `avg()` or `mean()`: computes average values for each numeric columns for each group\n- `pivot()`: pivots a column of the current DataFrame and perform the specified aggregation\n\nBefore we get into aggregations, let's load in a **CSV** with interesting data and create a new DataFrame.\n\nYou do this with the `spark-csv` package. Documentation on that is available at:\n- https://github.com/databricks/spark-csv\n\nThe dataset that will be loaded in to demonstrate contains data about flights departing New York City airports (`JFK`, `LGA`, `EWR`) in 2013. It has 336,776 rows and 16 columns."],"metadata":{}},{"cell_type":"markdown","source":["###Useful Links"],"metadata":{}},{"cell_type":"markdown","source":["There are many more functions... although I tried to cover a lot of ground, there are dozens more functions for DataFrames that I haven't touched upon.\n\nThe main project page for Spark:\n\n- http://spark.apache.org\n\nThe main reference for PySpark is:\n\n- http://spark.apache.org/docs/latest/api/python/index.html\n\nThese examples are available at:\n\n- https://github.com/rich-iannone/so-many-pyspark-examples\n\nInformation on the Parquet file format can be found at its project page:\n\n- http://parquet.apache.org\n\nThe GitHub project page for `spark-csv` package; contains usage documentation:\n\n- https://github.com/databricks/spark-csv"],"metadata":{}}],"metadata":{"name":"spark-dataframes","notebookId":96485},"nbformat":4,"nbformat_minor":0}