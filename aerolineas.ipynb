{"cells":[{"cell_type":"markdown","source":["##Spark DataFrames"],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import *  # Necessary for creating schemas\n","from pyspark.sql.functions import * # Importing PySpark functions"]},{"cell_type":"markdown","source":["###The 'groupBy' Function and Aggregations\n\nThe `groupBy()` function groups the DataFrame using the specified columns, then, we can run aggregation on them. The available aggregate functions are:\n\n- `count()`: counts the number of records for each group\n- `sum()`: compute the sum for each numeric column for each group\n- `min()`: computes the minimum value for each numeric column for each group\n- `max()`: computes the maximum value for each numeric column for each group\n- `avg()` or `mean()`: computes average values for each numeric columns for each group\n- `pivot()`: pivots a column of the current DataFrame and perform the specified aggregation\n\nBefore we get into aggregations, let's load in a **CSV** with interesting data and create a new DataFrame.\n\nYou do this with the `spark-csv` package. Documentation on that is available at:\n- https://github.com/databricks/spark-csv\n\nThe dataset that will be loaded in to demonstrate contains data about flights departing New York City airports (`JFK`, `LGA`, `EWR`) in 2013. It has 336,776 rows and 16 columns."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object...\n","nycflights_schema = StructType([\n","  StructField('year', IntegerType(), True),\n","  StructField('month', IntegerType(), True),\n","  StructField('day', IntegerType(), True),\n","  StructField('dep_time', StringType(), True),\n","  StructField('dep_delay', IntegerType(), True),\n","  StructField('arr_time', StringType(), True),\n","  StructField('arr_delay', IntegerType(), True),\n","  StructField('carrier', StringType(), True),\n","  StructField('tailnum', StringType(), True),\n","  StructField('flight', StringType(), True),  \n","  StructField('origin', StringType(), True),\n","  StructField('dest', StringType(), True),\n","  StructField('air_time', IntegerType(), True),\n","  StructField('distance', IntegerType(), True),\n","  StructField('hour', IntegerType(), True),\n","  StructField('minute', IntegerType(), True)\n","  ])\n","\n","# ...and then read the CSV with the schema\n","nycflights = \\\n","(sqlContext\n"," .read\n"," .format('com.databricks.spark.csv')\n"," .schema(nycflights_schema)\n"," .options(header = True)\n"," .load('s3://bigdatatoolscloud/nycflights13.csv'))\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Have a look at the schema for the imported dataset\nnycflights.printSchema()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights.show(5)"]},{"cell_type":"code","source":["# Let's group and aggregate\n\n# `groupBy()` will group one or more DF columns\n# and prep them for aggregration functions\n(nycflights\n .groupby('month') # creates 'GroupedData'\n .count() # creates a new column with aggregate `count` values\n .show())\n\n# Use the `agg()` function to perform multiple\n# aggregations\n(nycflights\n .groupby('month')\n .agg({'dep_delay': 'avg', 'arr_delay': 'avg'}) # note the new column names\n .show())\n\n# Caveat: you can't perform multiple aggregrations\n# on the same column (only the last is performed)\n(nycflights\n .groupby('month')\n .agg({'dep_delay': 'min', 'dep_delay': 'max'})\n .show())"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["###Column Operations\n\n`Column` instances can be created by:\n\n(1) Selecting a column from a DataFrame\n- `df.colName`\n- `df[\"colName\"]`\n- `df.select(df.colName)`\n- `df.withColumn(df.colName)`\n\n(2) Creating one from an expression\n- `df.colName + 1`\n- `1 / df.colName`\n\nOnce you have a `Column` instance, you can apply a wide range of functions. Some of the functions covered here are:\n- `format_number()`: apply formatting to a number, rounded to `d` decimal places, and return the result as a string\n- `when()` & `otherwise()`: `when()` evaluates a list of conditions and returns one of multiple possible result expressions; if `otherwise()` is not invoked, `None` is returned for unmatched conditions\n- `concat_ws()`: concatenates multiple input string columns together into a single string column, using the given separator\n- `to_utc_timestamp()`: assumes the given timestamp is in given timezone and converts to UTC\n- `year()`: extracts the year of a given date as integer\n- `month()`: extracts the month of a given date as integer\n- `dayofmonth()`: extracts the day of the month of a given date as integer\n- `hour()`: extract the hour of a given date as integer\n- `minute()`: extract the minute of a given date as integer"],"metadata":{}},{"cell_type":"code","source":["# Create a proper timestamp for once in your life...\n# We have all the components: `year`, `month`, `day`,\n# `hour`, and `minute`\n\n# Use `concat_ws()` (concatentate with separator) to\n# combine column data into StringType columns such\n# that dates (`-` separator, YYYY-MM-DD) and times\n# (`:` separator, 24-hour time) are formed\nnycflights = \\\n(nycflights\n .withColumn('date',\n             concat_ws('-',\n                       nycflights.year,\n                       nycflights.month,\n                       nycflights.day))\n .withColumn('time',\n             concat_ws(':',\n                       nycflights.hour,\n                       nycflights.minute)))\n\n# In a second step, concatenate with `concat_ws()`\n# the `date` and `time` strings (separator is a space);\n# then drop several columns\nnycflights = \\\n(nycflights\n .withColumn('timestamp',\n             concat_ws(' ',\n                       nycflights.date,\n                       nycflights.time))\n .drop('year')     # `drop()` doesn't accept\n .drop('month')    # a list of column names,\n .drop('day')      # therefore, for every column\n .drop('hour')     # we would like to remove\n .drop('minute')   # from the DataFrame, we \n .drop('date')     # must create a new `drop()`\n .drop('time'))    # statement\n\n# In the final step, convert the `timestamp` from\n# a StringType into a TimestampType\nnycflights = \\\n(nycflights\n .withColumn('timestamp',\n             to_utc_timestamp(nycflights.timestamp, 'GMT')))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights.show(5)"]},{"cell_type":"code","source":["# It probably doesn't matter in the end, but,\n# I'd prefer that the `timestamp` column be\n# the first column; let's make use of the\n# `columns` method and get slicing!\nnycflights = \\\n (nycflights\n  .select(nycflights.columns[-1:] + nycflights.columns[0:-1])) # recall that `columns` returns a list of column names"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Inspect the DataFrame's schema, note that `timestamp` is indeed classed as a timestamp\nnycflights.printSchema()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# WEATHER"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a schema object...\n","weather_schema = StructType([  \n","  StructField('year', IntegerType(), True),\n","  StructField('month', IntegerType(), True),\n","  StructField('day', IntegerType(), True),\n","  StructField('hour', IntegerType(), True),\n","  StructField('temp', FloatType(), True),\n","  StructField('dewp', FloatType(), True),\n","  StructField('humid', FloatType(), True),\n","  StructField('wind_dir', IntegerType(), True),\n","  StructField('wind_speed', FloatType(), True),\n","  StructField('wind_gust', FloatType(), True),\n","  StructField('precip', FloatType(), True),\n","  StructField('pressure', FloatType(), True),\n","  StructField('visib', FloatType(), True)\n","  ])\n","\n","#...and then read the CSV with the schema\n","weather = \\\n","(sqlContext\n"," .read\n"," .format('com.databricks.spark.csv')\n"," .schema(weather_schema)\n"," .options(header = True)\n"," .load('s3://bigdatatoolscloud/weather.csv'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We need those `month`, `day`, and `hour` values back\n","nycflights = \\\n","(nycflights\n"," .withColumn('month', month(nycflights.timestamp))\n"," .withColumn('day', dayofmonth(nycflights.timestamp))\n"," .withColumn('hour', hour(nycflights.timestamp)))\n","\n","# Join the `nycflights` DF with the `weather` DF \n","nycflights_all_columns = \\\n","(nycflights\n"," .join(weather,\n","       [nycflights.month == weather.month, # three join conditions: month,\n","        nycflights.day == weather.day,     #                        day,\n","        nycflights.hour == weather.hour],  #                        hour\n","       'left_outer')) # left outer join: keep all rows from the left DF (flights), with the matching rows in the right DF (weather)\n","                      # NULLs created if there is no match to the right DF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights_all_columns.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights_all_columns.show(5)"]},{"cell_type":"code","source":["# One way to reduce the number of extraneous\n# columns is to use a `select()` statement\nnycflights_wind_visib = \\\n(nycflights_all_columns\n .select(['timestamp', 'carrier', 'flight',\n          'origin', 'dest', 'wind_dir',\n          'wind_speed', 'wind_gust', 'visib']))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights_wind_visib.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights_wind_visib.show(5)"]},{"cell_type":"markdown","source":["Let's load in even more data so we can determine if any takeoffs occurred in very windy weather.\n\nThe **CSV** `beaufort_land.csv` contains Beaufort scale values (the `force` column), wind speed ranges in *mph*, and the name for each wind force."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object... \n","beaufort_land_schema = StructType([  \n","  StructField('force', IntegerType(), True),\n","  StructField('speed_mi_h_lb', IntegerType(), True),\n","  StructField('speed_mi_h_ub', IntegerType(), True),\n","  StructField('name', StringType(), True)\n","  ])\n","\n","# ...and then read the CSV with the schema\n","beaufort_land = \\\n","(sqlContext\n"," .read\n"," .format('com.databricks.spark.csv')\n"," .schema(beaufort_land_schema)\n"," .options(header = True)\n"," .load('s3://bigdatatoolscloud/beaufort_land.csv'))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["beaufort_land.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["beaufort_land.show(5)"]},{"cell_type":"code","source":["# Join the current working DF with the `beaufort_land` DF\n# and use join expressions that use the WS ranges\nnycflights_wind_visib_beaufort = \\\n(nycflights_wind_visib\n .join(beaufort_land,\n      [nycflights_wind_visib.wind_speed >= beaufort_land.speed_mi_h_lb,\n       nycflights_wind_visib.wind_speed < beaufort_land.speed_mi_h_ub],\n       'left_outer')\n .withColumn('month', month(nycflights_wind_visib.timestamp)) # Create a month column from `timestamp` values\n .drop('speed_mi_h_lb')\n .drop('speed_mi_h_ub')\n)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights_wind_visib_beaufort.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nycflights_wind_visib_beaufort.show(5)"]},{"cell_type":"code","source":["# We can inspect the number of potentially dangerous\n# takeoffs (i.e., where the Beaufort force is high)\n# month-by-month through the use of the `crosstab()` function\ncrosstab_month_force = \\\n(nycflights_wind_visib_beaufort\n .crosstab('month', 'force'))\n\n# After creating the crosstab DataFrame, use a few\n# functions to clean up the resultant DataFrame\ncrosstab_month_force = \\\n(crosstab_month_force\n .withColumn('month_force',\n             crosstab_month_force.month_force.cast('int')) # the column is initially a string but recasting as\n                                                           # an `int` will aid ordering in the next expression\n .orderBy('month_force')\n .drop('null'))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crosstab_month_force.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crosstab_month_force.show(5)"]},{"cell_type":"markdown","source":["###User Defined Functions (UDFs)\n\n**UDF**s allow for computations of values while looking at every input row in the DataFrame. They allow you to make your own function and import functionality from other **Python** libraries."],"metadata":{}},{"cell_type":"code","source":["# Define a function to convert velocity from\n# miles per hour (mph) to meters per second (mps)\ndef mph_to_mps(mph):\n  mps = mph * 0.44704\n  return mps\n\n# Register this function as a UDF using `udf()`\nmph_to_mps = udf(mph_to_mps, FloatType()) # An output type was specified"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["###Writing DataFrames to Files\nWe can easily write DataFrame data to a variety of different file formats."],"metadata":{}},{"cell_type":"code","source":["# Saving to CSV is quite similar to reading from a CSV file\n","(crosstab_month_force\n"," .write\n"," .mode('overwrite')\n"," .format('com.databricks.spark.csv')\n"," .save('s3://bigdatatoolscloud/crosstab_month_force.csv'))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["# Saving to Parquet is generally recommended for later retrieval\n","(crosstab_month_force\n"," .write\n"," .mode('overwrite')\n"," .parquet('s3://bigdatatoolscloud/crosstab_month_force.parquet'))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["###Useful Links"],"metadata":{}},{"cell_type":"markdown","source":["There are many more functions... although I tried to cover a lot of ground, there are dozens more functions for DataFrames that I haven't touched upon.\n\nThe main project page for Spark:\n\n- http://spark.apache.org\n\nThe main reference for PySpark is:\n\n- http://spark.apache.org/docs/latest/api/python/index.html\n\nThese examples are available at:\n\n- https://github.com/rich-iannone/so-many-pyspark-examples\n\nInformation on the Parquet file format can be found at its project page:\n\n- http://parquet.apache.org\n\nThe GitHub project page for `spark-csv` package; contains usage documentation:\n\n- https://github.com/databricks/spark-csv"],"metadata":{}}],"metadata":{"name":"spark-dataframes","notebookId":96485},"nbformat":4,"nbformat_minor":0}